{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import os\n",
    "logdir = '/tmp/debiased_classifier/'\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.'\n",
    "traindata_url = url + 'data'\n",
    "testdata_url = url + 'test'\n",
    "trainfile = urllib.request.urlopen(traindata_url)\n",
    "testfile = urllib.request.urlopen(testdata_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterbatches(arrays, *, num_batches=None, batch_size=None, shuffle=True,\n",
    "                include_final_partial_batch=True, repeat=False):\n",
    "    assert (num_batches is None) != (batch_size is None), 'Provide num_batches or batch_size, but not both'\n",
    "    arrays = tuple(map(np.asarray, arrays))\n",
    "    n = arrays[0].shape[0]\n",
    "    assert all(a.shape[0] == n for a in arrays[1:])\n",
    "    inds = np.arange(n)\n",
    "    first_time = True\n",
    "    while first_time or repeat:\n",
    "        if shuffle: np.random.shuffle(inds)\n",
    "        sections = np.arange(0, n, batch_size)[1:] if num_batches is None else num_batches\n",
    "        for batch_inds in np.array_split(inds, sections):\n",
    "            if include_final_partial_batch or len(batch_inds) == batch_size:\n",
    "                yield tuple(a[batch_inds] for a in arrays)\n",
    "        first_time = False\n",
    "\n",
    "def _weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.get_variable(\"W\", initializer=initial)\n",
    "\n",
    "def _bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.get_variable(\"b\", initializer=initial)\n",
    "\n",
    "def _linear(x, output_size, name):\n",
    "    with tf.variable_scope(name):\n",
    "        W = _weight_variable([x.get_shape().as_list()[1], output_size])\n",
    "        b = _bias_variable([output_size])\n",
    "        output = tf.matmul(x, W) + b\n",
    "    return output\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\n",
    "         \"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\n",
    "         \"capital-gain\",\"capital-loss\", \"hours-per-week\",\n",
    "         \"native-country\",\"income>50k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = pd.read_csv(trainfile, names=names, index_col=False, comment='|')\n",
    "raw_test_df = pd.read_csv(testfile, names=names, index_col=False, comment='|')\n",
    "\n",
    "train_df = pd.get_dummies(raw_train_df.drop(['fnlwgt', 'sex', 'income>50k'], axis=1))\n",
    "test_df = pd.get_dummies(raw_test_df.drop(['fnlwgt', 'sex', 'income>50k'], axis=1))\n",
    "extra_columns = list(set(train_df.columns) - set(test_df.columns))\n",
    "for c in extra_columns:\n",
    "    test_df[c] = 0\n",
    "    \n",
    "train_data = train_df.values\n",
    "train_labels, _ = raw_train_df['income>50k'].factorize()\n",
    "train_protected, _ = raw_train_df['sex'].factorize() # male = 0, female = 1\n",
    "test_data = test_df.values\n",
    "test_labels, _ = raw_test_df['income>50k'].factorize()\n",
    "test_protected, _ = raw_test_df['sex'].factorize() # male = 0, female = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240809557446\n",
      "0.236226276027\n"
     ]
    }
   ],
   "source": [
    "print(\"pct women in training set: {}\".format(train_labels.mean()))\n",
    "print(\"pct women in test set: {}\".format(test_labels.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamda = 1\n",
    "stepsize = 1e-4\n",
    "N_EPOCHS = 100\n",
    "batchsize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sw = tf.summary.FileWriter(logdir)\n",
    "data_input = tf.placeholder(tf.float32, shape=[None, train_data.shape[1]])\n",
    "protected_input = tf.placeholder(tf.int32, shape=[None])\n",
    "label_input = tf.placeholder(tf.int32, shape=[None])\n",
    "with tf.variable_scope(\"business\") as scope:\n",
    "    x = data_input\n",
    "    x = lrelu(_linear(x, 128, \"fc1\"))\n",
    "    x = lrelu(_linear(x, 128, \"fc2\"))\n",
    "    embedding = x\n",
    "    y_logits_ = tf.squeeze(_linear(embedding, 1, \"fc3\"), axis=1)\n",
    "    y_ = tf.sigmoid(y_logits_)\n",
    "    label_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(label_input, tf.float32),\n",
    "                                                                        logits=y_logits_))\n",
    "    label_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.round(y_), tf.int32), \n",
    "                                                     label_input), tf.float32))\n",
    "with tf.variable_scope(\"regulator\") as scope:\n",
    "    s_logits_ = tf.squeeze(_linear(embedding, 1, \"fc3\"), axis=1)\n",
    "    s_ = tf.sigmoid(s_logits_)\n",
    "    protected_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(protected_input, tf.float32),\n",
    "                                                                            logits=s_logits_))\n",
    "    protected_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.round(s_), tf.int32),\n",
    "                                                         protected_input), tf.float32))\n",
    "business_weights  = [var for var in tf.global_variables() if var.name[:8] == \"business\"]\n",
    "regulator_weights = [var for var in tf.global_variables() if var.name[:9] == \"regulator\"]\n",
    "#reg_loss = tf.reduce_sum([tf.nn.l2_loss(w) for w in weights])\n",
    "total_loss = label_loss - lamda*protected_loss\n",
    "train_business_op  = tf.train.AdamOptimizer(stepsize).minimize( total_loss, var_list=business_weights)\n",
    "train_regulator_op = tf.train.AdamOptimizer(stepsize).minimize(protected_loss, var_list=regulator_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_male, y_female = tf.dynamic_partition(data=y_, partitions=protected_input, num_partitions=2)\n",
    "discrimination_metric = tf.abs(tf.reduce_mean(y_male) - tf.reduce_mean(y_female))\n",
    "train_business_pct = tf.Variable(0.0)\n",
    "train_regulator_pct = tf.Variable(0.0)\n",
    "\n",
    "# training losses\n",
    "train_summary_ops = []\n",
    "val_summary_ops = []\n",
    "losses = [discrimination_metric, label_accuracy, label_loss, protected_accuracy, \n",
    "          protected_loss, train_business_pct, train_regulator_pct]\n",
    "loss_names = [\"discrimination_metric\", \"label_acc\", \"label_loss\", \"protected_acc\", \"protected_loss\",\n",
    "              \"train_business_pct\", \"train_regulator_pct\"]\n",
    "for pair in zip(losses, loss_names):\n",
    "    train_summary_ops.append(tf.summary.scalar(tensor=pair[0], name=\"training/\" + pair[1]))\n",
    "    val_summary_ops.append(tf.summary.scalar(tensor=pair[0], name=\"validation/\" + pair[1]))\n",
    "train_summary_op = tf.summary.merge(train_summary_ops)\n",
    "val_summary_op = tf.summary.merge(val_summary_ops)\n",
    "\n",
    "global_step = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "To log, call\n",
      " tensorboard --logdir=/tmp/debiased_classifier/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b78ffaba410c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             s_acc, train_summary, gs, *_ = sess.run(\n\u001b[1;32m     29\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mprotected_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mupdate_ops\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0msw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mshould_train_business\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0ms_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yonadav/anaconda/envs/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yonadav/anaconda/envs/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yonadav/anaconda/envs/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/yonadav/anaconda/envs/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yonadav/anaconda/envs/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                 run_metadata):\n\u001b[1;32m   1116\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m/Users/yonadav/anaconda/envs/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1166\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    should_train_business  = True\n",
    "    should_train_regulator = True\n",
    "    print(\"Starting training\")\n",
    "    print(\"To log, call\\n tensorboard --logdir={}\".format(logdir))\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_business_time = train_regulator_time = 0\n",
    "        for i, batch in enumerate(iterbatches(arrays=[train_data, train_labels, train_protected], \n",
    "                                              batch_size=batchsize, shuffle=True)):\n",
    "            data_batch, label_batch, protected_batch = batch\n",
    "            feed_dict = {\n",
    "                data_input: data_batch,\n",
    "                label_input: label_batch,\n",
    "                protected_input: protected_batch,\n",
    "            }\n",
    "            train_ops = []\n",
    "            \n",
    "            if should_train_business: train_ops.append(train_business_op)\n",
    "            if should_train_regulator: train_ops.append(train_regulator_op)\n",
    "            train_business_time += should_train_business\n",
    "            train_regulator_time += should_train_regulator\n",
    "            update_ops = [tf.assign(train_business_pct,  train_business_time/(i+1)), \n",
    "                         tf.assign(train_regulator_pct, train_regulator_time/(i+1)),\n",
    "                         tf.assign_add(global_step, data_batch.shape[0])]\n",
    "            \n",
    "            s_acc, train_summary, gs, *_ = sess.run(\n",
    "                [protected_accuracy, train_summary_op, global_step] + update_ops + train_ops,\n",
    "                feed_dict=feed_dict)\n",
    "            sw.add_summary(train_summary, gs)\n",
    "            should_train_business  = s_acc.mean() > 0.6\n",
    "            should_train_regulator = s_acc.mean() < 0.9\n",
    "        \n",
    "        valbatch = next(iterbatches(arrays=[test_data, test_labels, test_protected], \n",
    "                                    batch_size=512, shuffle=True))\n",
    "        data_batch, label_batch, protected_batch = valbatch\n",
    "        feed_dict = {\n",
    "            data_input: data_batch,\n",
    "            label_input: label_batch,\n",
    "            protected_input: protected_batch,\n",
    "        }\n",
    "        val_summary = sess.run(val_summary_op, feed_dict=feed_dict)\n",
    "        sw.add_summary(val_summary, gs)\n",
    "        sw.flush()\n",
    "        saver.save(sess, os.path.join(logdir, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
