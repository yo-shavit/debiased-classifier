{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an unbiased Adversarial Classifier\n",
    "This notebook is an implementation of the adversarially-unbiased classifier idea. In particular, this recreates the experiment from \"[Censoring Representations With an Adversary](https://arxiv.org/pdf/1511.05897.pdf)\", Edwards and Storkey, ICLR 2016.\n",
    "\n",
    "The experiment uses the UCI 'Adult' dataset, which contains a bunch of individual information (including sex). Our goal is to predict the label class, in this case whether or not the individual made more than $50k a year. Simultaneously, we will train our classifier to be blind to the individual's sex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import os\n",
    "logdir = '/tmp/debiased_classifier/'\n",
    "\n",
    "# Download the 'Adult' dataset from the UCI dataset archive\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.'\n",
    "traindata_url = url + 'data'\n",
    "testdata_url = url + 'test'\n",
    "trainfile = urllib.request.urlopen(traindata_url)\n",
    "testfile = urllib.request.urlopen(testdata_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are a few utility functions for the tensorflow part of this implementation\n",
    "\n",
    "def iterbatches(arrays, *, num_batches=None, batch_size=None, shuffle=True,\n",
    "                include_final_partial_batch=True, repeat=False):\n",
    "    assert (num_batches is None) != (batch_size is None), 'Provide num_batches or batch_size, but not both'\n",
    "    arrays = tuple(map(np.asarray, arrays))\n",
    "    n = arrays[0].shape[0]\n",
    "    assert all(a.shape[0] == n for a in arrays[1:])\n",
    "    inds = np.arange(n)\n",
    "    first_time = True\n",
    "    while first_time or repeat:\n",
    "        if shuffle: np.random.shuffle(inds)\n",
    "        sections = np.arange(0, n, batch_size)[1:] if num_batches is None else num_batches\n",
    "        for batch_inds in np.array_split(inds, sections):\n",
    "            if include_final_partial_batch or len(batch_inds) == batch_size:\n",
    "                yield tuple(a[batch_inds] for a in arrays)\n",
    "        first_time = False\n",
    "\n",
    "def _weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.get_variable(\"W\", initializer=initial)\n",
    "\n",
    "def _bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.get_variable(\"b\", initializer=initial)\n",
    "\n",
    "def _linear(x, output_size, name):\n",
    "    with tf.variable_scope(name):\n",
    "        W = _weight_variable([x.get_shape().as_list()[1], output_size])\n",
    "        b = _bias_variable([output_size])\n",
    "        output = tf.matmul(x, W) + b\n",
    "    return output\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The category names for the dataset\n",
    "names = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\n",
    "         \"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\n",
    "         \"capital-gain\",\"capital-loss\", \"hours-per-week\",\n",
    "         \"native-country\",\"income>50k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Data downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Loading and processing the data.\n",
    "\n",
    "print(\"Downloading data...\")\n",
    "raw_train_df = pd.read_csv(trainfile, names=names, index_col=False, comment='|')\n",
    "raw_test_df = pd.read_csv(testfile, names=names, index_col=False, comment='|')\n",
    "print(\"Data downloaded!\")\n",
    "\n",
    "# We discard 'sex' (which is our protected variable) and 'income>50k' (which is our predicted variable)\n",
    "# 'fnlwgt' is a scalar relating to the demographic importance of the individual, so we discard it too\n",
    "train_df = pd.get_dummies(raw_train_df.drop(['fnlwgt', 'sex', 'income>50k'], axis=1))\n",
    "test_df = pd.get_dummies(raw_test_df.drop(['fnlwgt', 'sex', 'income>50k'], axis=1))\n",
    "extra_columns = list(set(train_df.columns) - set(test_df.columns))\n",
    "for c in extra_columns:\n",
    "    test_df[c] = 0\n",
    "    \n",
    "train_data = train_df.values\n",
    "train_labels, _ = raw_train_df['income>50k'].factorize()\n",
    "train_protected, _ = raw_train_df['sex'].factorize() # male = 0, female = 1\n",
    "test_data = test_df.values\n",
    "test_labels, _ = raw_test_df['income>50k'].factorize()\n",
    "test_protected, _ = raw_test_df['sex'].factorize() # male = 0, female = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of women in training set: 0.2408\n",
      "fraction of women in test set: 0.2362\n"
     ]
    }
   ],
   "source": [
    "print(\"fraction of women in training set: {:0.4}\".format(train_labels.mean()))\n",
    "print(\"fraction of women in test set: {:0.4}\".format(test_labels.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamda = 1 # relative weight between \"classifying correctly\" and \"blinding the adversary\"\n",
    "stepsize = 1e-4\n",
    "N_EPOCHS = 100\n",
    "batchsize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sw = tf.summary.FileWriter(logdir)\n",
    "\n",
    "# inputs\n",
    "data_input = tf.placeholder(tf.float32, shape=[None, train_data.shape[1]]) # individual data\n",
    "protected_input = tf.placeholder(tf.int32, shape=[None]) # gender data\n",
    "label_input = tf.placeholder(tf.int32, shape=[None]) # over/under 50k salary data\n",
    "\n",
    "# First we create the primary classifier network (referred to as the \"business\")\n",
    "# It has 3 fully-connected layers with a sigmoid output.\n",
    "with tf.variable_scope(\"business\") as scope:\n",
    "    x = data_input\n",
    "    x = lrelu(_linear(x, 128, \"fc1\"))\n",
    "    x = lrelu(_linear(x, 128, \"fc2\"))\n",
    "    embedding = x\n",
    "    y_logits_ = tf.squeeze(_linear(embedding, 1, \"fc3\"), axis=1)\n",
    "    y_ = tf.sigmoid(y_logits_)\n",
    "    # The label loss is the cross-entropy classifiction loss for whether the individual was over/under 50k\n",
    "    label_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(label_input, tf.float32),\n",
    "                                                                        logits=y_logits_))\n",
    "    # Label accuracy is the percent of samples on which the predicted likelier outcome is the correct outcome\n",
    "    label_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.round(y_), tf.int32), \n",
    "                                                     label_input), tf.float32))\n",
    "\n",
    "# We also create an adversary \"regulator\", whose goal is to predict the protected attribute\n",
    "# if she can, she knows the business is biased\n",
    "with tf.variable_scope(\"regulator\") as scope:\n",
    "    s_logits_ = tf.squeeze(_linear(embedding, 1, \"fc3\"), axis=1)\n",
    "    s_ = tf.sigmoid(s_logits_)\n",
    "    # protected loss is the cross-entropy classification loss for gender prediction\n",
    "    protected_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(protected_input, tf.float32),\n",
    "                                                                            logits=s_logits_))\n",
    "    protected_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.round(s_), tf.int32),\n",
    "                                                         protected_input), tf.float32))\n",
    "# The business can only update the primary classifier's weights, \n",
    "# and the regulator can only update the protected-attribute classifier weights\n",
    "business_weights  = [var for var in tf.global_variables() if var.name[:8] == \"business\"]\n",
    "regulator_weights = [var for var in tf.global_variables() if var.name[:9] == \"regulator\"]\n",
    "\n",
    "#reg_loss = tf.reduce_sum([tf.nn.l2_loss(w) for w in weights])\n",
    "total_loss = label_loss - lamda*protected_loss\n",
    "# The business minimizes its normal label-prediction loss, while also maximizing the regulator's prediction loss\n",
    "train_business_op  = tf.train.AdamOptimizer(stepsize).minimize(total_loss, var_list=business_weights)\n",
    "# The regulator only wishes to minimize its own prediction loss\n",
    "train_regulator_op = tf.train.AdamOptimizer(stepsize).minimize(protected_loss, var_list=regulator_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_male, y_female = tf.dynamic_partition(data=y_, partitions=protected_input, num_partitions=2)\n",
    "# The discrimination metric, as defined by Edwards and Storkey, is sort of a measure of the average difference\n",
    "# in prediction rates across gender - which they call 'discrimination'. This is of course a lacking definition, but\n",
    "# this notebook is just about reimplementing their paper.\n",
    "discrimination_metric = tf.abs(tf.reduce_mean(y_male) - tf.reduce_mean(y_female))\n",
    "\n",
    "# Because the two networks (business and regulator) are competing with each other, we have to carefully balance their training.\n",
    "# If the regulator gets too good, it will just force the business to purge all information so the regulator sees nothing.\n",
    "# If the business gets too good, the regulator will never get a chance to start identifying sensitive attributes, \n",
    "# leading to a sexist classifier\n",
    "# In the paper, they propose sometimes training the regulator \n",
    "# (if the regulator's classification accuracy is low, below 0.9),\n",
    "# and sometimes training the business (if the regulator's classification accuracy is high, above 0.6)\n",
    "# These two variabels capture the pct time spent training each\n",
    "train_business_pct = tf.Variable(0.0)\n",
    "train_regulator_pct = tf.Variable(0.0)\n",
    "\n",
    "# training losses\n",
    "train_summary_ops = []\n",
    "val_summary_ops = []\n",
    "losses = [discrimination_metric, label_accuracy, label_loss, protected_accuracy, \n",
    "          protected_loss, train_business_pct, train_regulator_pct]\n",
    "loss_names = [\"discrimination_metric\", \"label_acc\", \"label_loss\", \"protected_acc\", \"protected_loss\",\n",
    "              \"train_business_pct\", \"train_regulator_pct\"]\n",
    "for pair in zip(losses, loss_names):\n",
    "    train_summary_ops.append(tf.summary.scalar(tensor=pair[0], name=\"training/\" + pair[1]))\n",
    "    val_summary_ops.append(tf.summary.scalar(tensor=pair[0], name=\"validation/\" + pair[1]))\n",
    "train_summary_op = tf.summary.merge(train_summary_ops)\n",
    "val_summary_op = tf.summary.merge(val_summary_ops)\n",
    "\n",
    "global_step = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "To log, call\n",
      " tensorboard --logdir=/tmp/debiased_classifier/\n",
      "open http://0.0.0.0:6006\n"
     ]
    }
   ],
   "source": [
    "# Here we actually run the training\n",
    "# We recommend visualizing the results in tensorboard\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    should_train_business  = True\n",
    "    should_train_regulator = True\n",
    "    print(\"Starting training\")\n",
    "    print(\"To log, call\\ntensorboard --logdir={}\".format(logdir))\n",
    "    print(\"open http://0.0.0.0:6006\")\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_business_time = train_regulator_time = 0\n",
    "        for i, batch in enumerate(iterbatches(arrays=[train_data, train_labels, train_protected], \n",
    "                                              batch_size=batchsize, shuffle=True)):\n",
    "            # load each data batch into the network\n",
    "            data_batch, label_batch, protected_batch = batch\n",
    "            feed_dict = {\n",
    "                data_input: data_batch,\n",
    "                label_input: label_batch,\n",
    "                protected_input: protected_batch,\n",
    "            }\n",
    "            train_ops = []\n",
    "            \n",
    "            if should_train_business: train_ops.append(train_business_op)\n",
    "            if should_train_regulator: train_ops.append(train_regulator_op)\n",
    "            train_business_time += should_train_business\n",
    "            train_regulator_time += should_train_regulator\n",
    "            update_ops = [tf.assign(train_business_pct,  train_business_time/(i+1)), \n",
    "                         tf.assign(train_regulator_pct, train_regulator_time/(i+1)),\n",
    "                         tf.assign_add(global_step, data_batch.shape[0])]\n",
    "            \n",
    "            # Train the network on a minibatch and update the weights of the network, plus bookkeeping.\n",
    "            s_acc, train_summary, gs, *_ = sess.run(\n",
    "                [protected_accuracy, train_summary_op, global_step] + update_ops + train_ops,\n",
    "                feed_dict=feed_dict)\n",
    "            sw.add_summary(train_summary, gs)\n",
    "            # Pick whether, in the next timestep, to train the business, regulator, or both\n",
    "            should_train_business  = s_acc.mean() > 0.6\n",
    "            should_train_regulator = s_acc.mean() < 0.9\n",
    "        \n",
    "        # Evaluate the network on a validation set\n",
    "        valbatch = next(iterbatches(arrays=[test_data, test_labels, test_protected], \n",
    "                                    batch_size=512, shuffle=True))\n",
    "        data_batch, label_batch, protected_batch = valbatch\n",
    "        feed_dict = {\n",
    "            data_input: data_batch,\n",
    "            label_input: label_batch,\n",
    "            protected_input: protected_batch,\n",
    "        }\n",
    "        val_summary = sess.run(val_summary_op, feed_dict=feed_dict)\n",
    "        sw.add_summary(val_summary, gs)\n",
    "        sw.flush()\n",
    "        # And finally, save the model weights (they'll be in /tmp/debiased_classifier/, unless you changed the logdir)\n",
    "        saver.save(sess, os.path.join(logdir, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
